# filter_api/api.py

import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from fastapi import APIRouter, HTTPException, status
from filter_api.model import FilterQueryRequest, FilterResponse, ErrorResponse
from dotenv import load_dotenv

load_dotenv()

# ============================================================
# ì „ì—­ ë³€ìˆ˜ (ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥)
# ============================================================

# ëª¨ë¸ ë¡œë“œ ìƒíƒœ ë° ê°ì²´ ì €ì¥
TOXICITY_MODEL = None
TOXICITY_TOKENIZER = None
DEVICE = "cpu" # ì´ˆê¸°ê°’, initialize_toxicity_modelì—ì„œ ë³€ê²½ë¨


print("DEBUG TOXIC_PATH=", os.getenv("TOXICITY_MODEL_PATH"))

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (í•„ìš”ì— ë”°ë¼ .envì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ í•˜ë“œì½”ë”©)
MODEL_DIR = os.getenv("TOXICITY_MODEL_PATH")
if not MODEL_DIR:
    raise RuntimeError("X TOXICITY_MODEL_PATH(.env)ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ============================================================
# 1) ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜
# ============================================================

def initialize_toxicity_model():
    """í´ë¦°ë´‡ AI ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³  ì „ì—­ ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤."""
    global TOXICITY_MODEL, TOXICITY_TOKENIZER, DEVICE
    
    # ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸ (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)
    if TOXICITY_MODEL is not None:
        print("â„¹ï¸ ìœ í•´ì„± í•„í„°ë§ ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # GPU/CPU ì„¤ì •
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"ğŸš€ ìœ í•´ì„± í•„í„°ë§ ëª¨ë¸ ë¡œë“œ ì¤‘... (DEVICE: {DEVICE})")
    
    # ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ
    TOXICITY_TOKENIZER = AutoTokenizer.from_pretrained(MODEL_DIR)
    TOXICITY_MODEL = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
    TOXICITY_MODEL.to(DEVICE)
    TOXICITY_MODEL.eval() # í‰ê°€ ëª¨ë“œ ì„¤ì •
    
    print("âœ… ìœ í•´ì„± í•„í„°ë§ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

# ============================================================
# 2) ì˜ˆì¸¡ í•¨ìˆ˜ (í´ë¦°ë´‡ ë¡œì§)
# ============================================================

def predict_toxicity(text: str) -> dict:
    """
    í…ìŠ¤íŠ¸ì˜ ìœ í•´ì„± í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. (í´ë¦°ë´‡ai.py ë¡œì§ ê¸°ë°˜)
    
    Returns: { "ì •ìƒ í™•ë¥ ": float, "ìœ í•´ í™•ë¥ ": float }
    """
    if TOXICITY_MODEL is None or TOXICITY_TOKENIZER is None:
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ì˜¤ë¥˜ ë°œìƒ
        raise RuntimeError("ìœ í•´ì„± í•„í„°ë§ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    # í† í°í™”
    inputs = TOXICITY_TOKENIZER(text, return_tensors="pt", truncation=True, padding=True)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    # ëª¨ë¸ ì˜ˆì¸¡
    with torch.no_grad():
        outputs = TOXICITY_MODEL(**inputs)

    logits = outputs.logits
    # Softmaxë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ê³„ì‚°. [0]ì€ ë°°ì¹˜ ì°¨ì› ì œê±°
    probs = torch.softmax(logits, dim=1).cpu().numpy()[0] 
    
    # ëª¨ë¸ì˜ ë¼ë²¨ ìˆœì„œì— ë”°ë¼ í™•ë¥ ì„ ë°˜í™˜ (0: ì •ìƒ, 1: ìœ í•´ë¼ê³  ê°€ì •)
    return {
        "ì •ìƒ í™•ë¥ ": probs[0],
        "ìœ í•´ í™•ë¥ ": probs[1]
    }

# ============================================================
# 3) API ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (ë”ë¯¸ ë°ì´í„° ì œê±° ë°˜ì˜)
# ============================================================

def create_filter_response(text: str) -> dict:
    """
    API ëª…ì„¸ì— ë§ëŠ” í•„í„°ë§ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # ìœ í•´ì„± ì˜ˆì¸¡
    prediction = predict_toxicity(text)
    
    # toxicityëŠ” ìœ í•´ í™•ë¥ ì„ ì‚¬ìš©
    toxicity_score = prediction["ìœ í•´ í™•ë¥ "]
    
    # blocked ê²°ì •: toxicity >= 0.7ì´ë©´ True (ì„ê³„ê°’ 0.7 ì‚¬ìš©)
    blocked = toxicity_score >= 0.7
    
    # ğŸš¨ ë”ë¯¸ ë°ì´í„° ë¡œì§ ì œê±°: ë‹¤ë¥¸ ì ìˆ˜ë“¤ì€ 0ìœ¼ë¡œ ì„¤ì •
    insult = 0.0
    profanity = 0.0
    hate = 0.0
    threat = 0.0
    
    return {
        "originQuery": text,
        "toxicity": round(toxicity_score, 4),
        "insult": insult,
        "profanity": profanity,
        "hate": hate,
        "threat": threat,
        "blocked": blocked
    }

# ============================================================
# 4) FastAPI ë¼ìš°í„° ì •ì˜
# ============================================================

# APIRouter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
router = APIRouter(
    prefix="/api/v1/filter",
    tags=["Toxicity Filter (Cleanbot AI)"]
)

@router.post(
    "/text", 
    response_model=FilterResponse,
    responses={
        400: {"model": ErrorResponse, "description": "í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëˆ„ë½"},
        500: {"model": ErrorResponse, "description": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ (ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨)"}
    },
    summary="í…ìŠ¤íŠ¸ ìœ í•´ì„± í•„í„°ë§",
    description="ì»¤ë®¤ë‹ˆí‹° ì½˜í…ì¸ (ê²Œì‹œê¸€/ëŒ“ê¸€)ì˜ ìœ í•´ì„±ì„ íƒì§€í•©ë‹ˆë‹¤."
)
def filter_text_endpoint(request: FilterQueryRequest):
    """
    í…ìŠ¤íŠ¸ ìœ í•´ì„± í•„í„°ë§ API
    """
    
    # query í•„ë“œ ê²€ì¦
    if not request.query or request.query.strip() == "":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "error": "BAD_REQUEST",
                "message": "í•„ìˆ˜ íŒŒë¼ë¯¸í„°ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "details": {"missing_fields": ["query"]}
            }
        )
    
    try:
        # ìœ í•´ì„± í•„í„°ë§ ì‹¤í–‰
        filter_result = create_filter_response(request.query)
        return FilterResponse(**filter_result)
        
    except RuntimeError as e:
        # ëª¨ë¸ ì´ˆê¸°í™” ê´€ë ¨ ì˜¤ë¥˜ëŠ” 500ìœ¼ë¡œ ì²˜ë¦¬
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "MODEL_NOT_INITIALIZED",
                "message": str(e)
            }
        )
    except Exception as e:
        # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"Filter API Error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "INTERNAL_SERVER_ERROR",
                "message": "ìœ í•´ì„± í•„í„°ë§ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
        )