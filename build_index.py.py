import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# ğŸ”§ ì„¤ì •
DATA_DIR = "./bizinfo_data"
INDEX_PATH = "./faiss_index.index"
METADATA_PATH = "./metadata.json"

# âœ… ëª¨ë¸ ë¡œë”©
model = SentenceTransformer("all-MiniLM-L6-v2")

# ğŸ“‚ ëª¨ë“  extracted.txt íŒŒì¼ ì°¾ê¸°
chunks = []
metadata = []

for root, dirs, files in os.walk(DATA_DIR):
    for file in files:
        if file == "extracted.txt":
            file_path = os.path.join(root, file)
            with open(file_path, encoding="utf-8") as f:
                text = f.read()

            # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
            title = os.path.basename(os.path.dirname(file_path))
            paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
            for p in paragraphs:
                chunks.append(p)
                metadata.append({
                    "title": title,
                    "source": "ë¡œì»¬",
                    "url": "",
                    "text": p
                })

# ğŸ“Œ ì„ë² ë”©
embeddings = model.encode(chunks).astype("float32")

# ğŸ” FAISS ì¸ë±ìŠ¤ ìƒì„±
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
faiss.write_index(index, INDEX_PATH)

# ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥
with open(METADATA_PATH, "w", encoding="utf-8") as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)

print(f"âœ… ì´ {len(chunks)} ê°œì˜ ë¬¸ë‹¨ì´ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.")
