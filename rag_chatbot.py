#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì •ì±… ì§€ì› ì•ˆë‚´ RAG ì±—ë´‡
- FAISS ì¸ë±ìŠ¤ ê¸°ë°˜ ê²€ìƒ‰
- OpenAI GPT-4oë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„±
- íŒŒì¸íŠœë‹ëœ BAAI/bge-m3 ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
"""

import os
import json
import faiss
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional # Optional import ì¶”ê°€
from openai import OpenAI, APIError # APIError import ì¶”ê°€
from transformers import AutoModel, AutoTokenizer
import torch

# ============================================================
# 1. ê²½ë¡œ ì„¤ì • (ğŸš¨ Path ê°ì²´ ëŒ€ì‹  ìˆœìˆ˜ ë¬¸ìì—´ë¡œ ìˆ˜ì •ë¨ ğŸš¨)
# ============================================================

# íŒŒì¼ ê²½ë¡œ
FINETUNED_MODEL_PATH = "C:\\Users\\user\\Desktop\\bge-m3-sft"
FAISS_INDEX_PATH = "C:\\Users\\user\\Desktop\\policy_faiss.index"
METADATA_PATH = "C:\\Users\\user\\Desktop\\metadata.json"

# OpenAI API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    print("âš ï¸  ê²½ê³ : OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. í„°ë¯¸ë„ì—ì„œ: export OPENAI_API_KEY='your-api-key'")
    print("2. ì½”ë“œì—ì„œ ì§ì ‘: OPENAI_API_KEY = 'your-api-key'")
    # ë˜ëŠ” ì—¬ê¸°ì— ì§ì ‘ ì…ë ¥ (ë³´ì•ˆìƒ ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
    # OPENAI_API_KEY = "your-api-key-here"

print("="*70)
print("ğŸ¤– ì •ì±… ì§€ì› ì•ˆë‚´ RAG ì±—ë´‡")
print("="*70)
# ì¶œë ¥ ì‹œì—ëŠ” Path ê°ì²´ì˜ .exists() ëŒ€ì‹  os.path.existsë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • í•„ìš”
print(f"ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {FINETUNED_MODEL_PATH}")
print(f"ğŸ“‚ FAISS ì¸ë±ìŠ¤: {FAISS_INDEX_PATH}")
print(f"ğŸ“‚ ë©”íƒ€ë°ì´í„°: {METADATA_PATH}")
print("="*70 + "\n")


# ============================================================
# 2. íŒŒì¸íŠœë‹ëœ ì„ë² ë”© ëª¨ë¸ ë¡œë” í´ë˜ìŠ¤
# ============================================================

class FineTunedEmbedder:
    """íŒŒì¸íŠœë‹ëœ BAAI/bge-m3 ì„ë² ë”© ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: str = None):
        """
        Args:
            model_path: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
            device: 'cuda' ë˜ëŠ” 'cpu' (Noneì´ë©´ ìë™ ê°ì§€)
        """
        print("ğŸ“¦ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        self.model = AutoModel.from_pretrained(model_path, local_files_only=True) # local_file_only ì˜¤íƒ€ ìˆ˜ì •
        self.model.to(self.device)
        self.model.eval()
        
        print(f"  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}\n")
    
    def encode(self, texts: List[str], batch_size: int = 32, max_length: int = 512) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            numpy array (n_texts, embedding_dim)
        """
        embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # í† í°í™”
                encoded = self.tokenizer(
                    batch_texts,
                    max_length=max_length,
                    padding=True,
                    truncation=True,
                    return_tensors='pt'
                )
                
                # GPUë¡œ ì´ë™
                input_ids = encoded['input_ids'].to(self.device)
                attention_mask = encoded['attention_mask'].to(self.device)
                
                # ì„ë² ë”© ìƒì„±
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                
                # CLS í† í° ì¶”ì¶œ ë° ì •ê·œí™”
                cls_embeddings = outputs.last_hidden_state[:, 0, :]
                cls_embeddings = torch.nn.functional.normalize(cls_embeddings, p=2, dim=1)
                
                # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
                embeddings.append(cls_embeddings.cpu().numpy())
        
        return np.vstack(embeddings)


# ============================================================
# 3. FAISS ê²€ìƒ‰ê¸° í´ë˜ìŠ¤
# ============================================================

class FAISSRetriever:
    """FAISS ì¸ë±ìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, index_path: str, metadata_path: str, embedder: FineTunedEmbedder):
        """
        Args:
            index_path: FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
            metadata_path: ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ
            embedder: ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        print("ğŸ“š FAISS ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self.index = faiss.read_index(str(index_path))
        print(f"  - ì¸ë±ìŠ¤ í¬ê¸°: {self.index.ntotal:,}ê°œ ë¬¸ì„œ")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        print(f"  - ë©”íƒ€ë°ì´í„°: {len(self.metadata):,}ê°œ í•­ëª©")
        
        # ì„ë² ë”© ëª¨ë¸
        self.embedder = embedder
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ FAISS ì¸ë±ìŠ¤ë¥¼ GPUë¡œ ì´ë™
        if torch.cuda.is_available() and faiss.get_num_gpus() > 0:
            print("  - FAISS GPU ëª¨ë“œ í™œì„±í™”")
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
        
        print("  âœ… FAISS ê²€ìƒ‰ê¸° ì¤€ë¹„ ì™„ë£Œ\n")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ top-k ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ metadata + score)
        """
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedder.encode([query])

        query_embedding = query_embedding.astype('float32')

        print(f"  ğŸ” ì¿¼ë¦¬ ë²¡í„° ì°¨ì›: {query_embedding.shape[1]}")
        print(f"  ğŸ” ì¿¼ë¦¬ ë²¡í„° Dtype: {query_embedding.dtype}")

        # FAISS ê²€ìƒ‰ (L2 ê±°ë¦¬)
        distances, indices = self.index.search(query_embedding, top_k)
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.metadata):
                result = self.metadata[idx].copy()
                result['similarity']=float(dist)
                result['score']=float(dist)
                results.append(result)
        
        return results


# ============================================================
# 4. RAG í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
# ============================================================

def create_rag_prompt(query: str, retrieved_docs: List[Dict[str, Any]]) -> str:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì¿¼ë¦¬ë¥¼ ê²°í•©í•˜ì—¬ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    MAX_CONTENT_LENGTH = 3000

    for i, doc in enumerate(retrieved_docs, 1):
        context_parts.append(f"[ë¬¸ì„œ {i}]")
        context_parts.append(f"ì œëª©: {doc.get('title', 'ì œëª© ì—†ìŒ')}")
        
        full_content = doc.get('content', doc.get('text', 'ë‚´ìš©ì—†ìŒ'))
        truncated_content = full_content[:MAX_CONTENT_LENGTH] + ("..." if len(full_content) > MAX_CONTENT_LENGTH else "")
        context_parts.append(f"ë‚´ìš©: {truncated_content}")
        
        # ì¶œì²˜ ì •ë³´ë¥¼ LLM í”„ë¡¬í”„íŠ¸ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
        # if 'source' in doc:
        #     context_parts.append(f"ì¶œì²˜: {doc['source']}")
        context_parts.append("") # ë¹ˆ ì¤„
    
    context = "\n".join(context_parts)
    
    # ğŸš¨ğŸš¨ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìˆ˜ì •: JSON ì¶œë ¥ ë° ë™ì  ì§ˆë¬¸ ìƒì„± ì§€ì‹œ ğŸš¨ğŸš¨
    system_instruction = f"""
    ë„ˆëŠ” ì •ì±…ì´ë‚˜ ë²•ë¥  ìš©ì–´ë¥¼ ì–´ë ¤ì›Œí•˜ëŠ” ì¹œêµ¬ì—ê²Œ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ì¹œì ˆí•œ ì •ì±… ì „ë¬¸ ì¡°ì–¸ìì•¼.
    
    ë„ˆì˜ ìµœì¢… ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê³ , ê´€ë ¨ëœ í›„ì† ì§ˆë¬¸ 4ê°œë¥¼ ìƒì„±í•˜ë©°, ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê±°ì•¼.
    
    --- LLM í–‰ë™ ë° ì‘ë‹µ ê·œì¹™ ---
    1. **ì‘ë‹µ í˜•ì‹:** ì‘ë‹µì€ ë°˜ë“œì‹œ **ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆ**ë¥¼ ë”°ë¼ì•¼ í•´.
       {{
         "botResponse": "ìƒì„±ëœ ìµœì¢… ë‹µë³€ í…ìŠ¤íŠ¸",
         "queryTitle": "ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ìš”ì•½ ì œëª©",
         "followUpQuestions": ["í›„ì† ì§ˆë¬¸ 1", "í›„ì† ì§ˆë¬¸ 2", "í›„ì† ì§ˆë¬¸ 3", "í›„ì† ì§ˆë¬¸ 4"]
       }}
    2. **ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼:** 'botResponse' í•„ë“œì— ë“¤ì–´ê°ˆ ë‹µë³€ì€ ë°˜ë“œì‹œ í•­ëª© ì œëª©(`ì§€ì› ëŒ€ìƒ:`, `ì‹ ì²­ ë°©ë²•:`)ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ê¸€ë¡œ ë…¹ì—¬ë‚´ì•¼ í•´. (ì¹œê·¼í•œ í¸ì§€ë‚˜ ë©”ì‹œì§€ í˜•íƒœ)
    3. **í˜ë¥´ì†Œë‚˜ì™€ ê°€ë…ì„±:** 'botResponse'ëŠ” 'ì¹œêµ¬ì²˜ëŸ¼', 'ì„ ë°°ì²˜ëŸ¼' ì¹œê·¼í•˜ê²Œ ì‘ì„±í•˜ê³ , ë¬¸ë‹¨ êµ¬ë¶„ì„ ëª…í™•íˆ í•˜ê³ , ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì—¬ ê°€ë…ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•´.
    4. **ë³¸ë¬¸ URL ê¸ˆì§€:** 'botResponse' ë³¸ë¬¸(ì‹¤í–‰ë ¥ í‚¤ìš°ê¸° ì´ì „)ì—ëŠ” **ì ˆëŒ€ë¡œ** URL, ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ, ë¬¸ì„œ ì œëª©, ì¶œì²˜ ë“±ì˜ ì •ë³´ë¥¼ **í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**
    5. **ì‹¤í–‰ë ¥ í‚¤ìš°ê¸°:** 'botResponse'ì˜ ë§ˆì§€ë§‰ì—ëŠ” ì†Œìƒê³µì¸ ì¹œêµ¬ê°€ ë°”ë¡œ ì›€ì§ì¼ ìˆ˜ ìˆë„ë¡, ê°€ì¥ ë¹ ë¥´ê³  êµ¬ì²´ì ì¸ ë‹¤ìŒ í–‰ë™ ë‹¨ê³„ë¥¼ ë”± í•˜ë‚˜ë§Œ ì½• ì§‘ì–´ ì œì‹œí•´ ì¤˜. ì´ ë‹¨ê³„ì—ëŠ” **ê°€ì¥ ì í•©í•œ ê³µê³ ë¬¸ URLì„ í•˜ì´í¼ë§í¬ í˜•ì‹([ê³µê³ ë¬¸ ë°”ë¡œê°€ê¸°](URL))ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ì²¨ë¶€**í•´ì•¼ í•´.
    6. **í›„ì† ì§ˆë¬¸:** 'followUpQuestions' í•„ë“œì— **ë‹µë³€ ë‚´ìš©ì— ê¸°ë°˜í•œ, ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´ í•  ë§Œí•œ 4ê°œì˜ ìƒˆë¡œìš´ ì§ˆë¬¸**ì„ ìƒì„±í•´.
    7. **ë¬¸ì„œ ê¸°ë°˜:** ì œê³µëœ ë¬¸ì„œì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë ¤ì›Œ. ë” ì°¾ì•„ë³´ì!"ë¼ê³  ì‘ë‹µí•˜ê³ , ì´ ê²½ìš° 'followUpQuestions'ì—ëŠ” ì„ì˜ì˜ 4ê°œ ì§ˆë¬¸ì„ ë„£ì–´.
    
    --- ë ---
    """
    
    # ğŸš¨ğŸš¨ ì‚¬ìš©ì ì½˜í…ì¸  (User Content) ì •ì˜ ğŸš¨ğŸš¨
    user_content = f"""
    --- ë‹µë³€ ì°¸ê³  ìë£Œ ---
    {context}
    
    ì‚¬ìš©ì ì§ˆë¬¸: {query}
    
    ë‹µë³€:
    """
    
    # SYSTEM ì—­í• ê³¼ USER ì—­í• ì˜ ë©”ì‹œì§€ë¥¼ ê²°í•©í•˜ì—¬ ë°˜í™˜ (API í˜¸ì¶œ ì‹œ ë¶„ë¦¬ë¨)
    return f"{system_instruction.strip()}\n{user_content.strip()}"


# ============================================================
# 5. OpenAI GPT-4o ì‘ë‹µ ìƒì„±ê¸° (JSON íŒŒì‹± ë¡œì§ í¬í•¨)
# ============================================================

class GPT4oGenerator:
    """OpenAI GPT-4o ê¸°ë°˜ ì‘ë‹µ ìƒì„±ê¸°"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        # ... (ì¤‘ëµ)
        self.client = OpenAI(api_key=api_key)
        self.model = model
        print(f"  - ëª¨ë¸: {model}")
        print("  âœ… ì¤€ë¹„ ì™„ë£Œ\n")
    
    def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1500, # JSON ì¶œë ¥ì„ ìœ„í•´ í† í° ì—¬ìœ ë¥¼ ë‘¡ë‹ˆë‹¤.
        stream: bool = False
    ) -> Optional[Dict[str, Any]]: # ë°˜í™˜ íƒ€ì…ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€ê²½
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„± (JSON í˜•ì‹ ê°•ì œ)
        """
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œëŠ” JSON ì¶œë ¥ê³¼ í˜¸í™˜ë˜ì§€ ì•Šì•„ ì—¬ê¸°ì„œëŠ” ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
            print("âŒ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œëŠ” JSON ì‘ë‹µ í˜•ì‹ê³¼ ë™ì‹œì— ì‚¬ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.")
            return None 

        try:
            # 1. í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬: ì‹œìŠ¤í…œ ì§€ì¹¨ê³¼ ì‚¬ìš©ì ì½˜í…ì¸ ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
            if "--- ë‹µë³€ ì°¸ê³  ìë£Œ ---" in prompt:
                system_content = prompt.split("--- ë‹µë³€ ì°¸ê³  ìë£Œ ---")[0].strip()
                user_content = prompt.split("--- ë‹µë³€ ì°¸ê³  ìë£Œ ---")[1].strip()
                
                messages = [
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": user_content}
                ]
            else:
                messages = [{"role": "user", "content": prompt}]

            # 2. API í˜¸ì¶œ ë° JSON í˜•ì‹ ê°•ì œ
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format={"type": "json_object"} # ğŸ‘ˆ JSON í˜•ì‹ ê°•ì œ
            )
            
            json_text = response.choices[0].message.content
            
            # 3. JSON íŒŒì‹±
            try:
                result_dict = json.loads(json_text)
                return result_dict
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                print(f"ë°›ì€ í…ìŠ¤íŠ¸: {json_text[:200]}...")
                return None
                
        except APIError as e:
            print(f"âŒ OpenAI API ì˜¤ë¥˜: {e.status_code} - {e.response.text}")
            return None
        except Exception as e:
            print(f"âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return None


# ============================================================
# 6. RAG ì±—ë´‡ í´ë˜ìŠ¤ (ì „ì²´ íŒŒì´í”„ë¼ì¸)
# ============================================================

class PolicyRAGChatbot:
    """ì •ì±… ì§€ì› ì•ˆë‚´ RAG ì±—ë´‡"""
    
    def __init__(
        self,
        model_path: str,
        index_path: str,
        metadata_path: str,
        api_key: str,
        device: str = None
    ):
        """
        Args:
            model_path: íŒŒì¸íŠœë‹ëœ ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ
            index_path: FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
            metadata_path: ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ
            api_key: OpenAI API í‚¤
            device: ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        """
        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.embedder = FineTunedEmbedder(str(model_path), device)
        
        # 2. FAISS ê²€ìƒ‰ê¸° ë¡œë“œ
        self.retriever = FAISSRetriever(index_path, metadata_path, self.embedder)
        
        # 3. GPT-4o ìƒì„±ê¸° ì´ˆê¸°í™”
        self.generator = GPT4oGenerator(api_key)
        
        print("="*70)
        print("âœ… RAG ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!")
        print("="*70 + "\n")
    
    def answer(
        self,
        query: str,
        top_k: int = 5,
        temperature: float = 0.7,
        stream: bool = False, # JSON ì¶œë ¥ ë•Œë¬¸ì— Falseë¡œ ê³ ì • (ì£¼ì„ìœ¼ë¡œ ì²˜ë¦¬)
        show_sources: bool = True
    ) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (FastAPI API ìŠ¤í™ ì¤€ìˆ˜)
        """
        print(f"ğŸ’­ ì§ˆë¬¸: {query}\n")
        
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        print(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘... (top-{top_k})")
        retrieved_docs = self.retriever.search(query, top_k)
        print(f"  âœ… {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ\n")
        
        # ì¶œì²˜ í‘œì‹œ (ë°±ì—”ë“œ ì½˜ì†” ì¶œë ¥ìš©)
        if show_sources:
            print("ğŸ“š ì°¸ê³  ë¬¸ì„œ:")
            for i, doc in enumerate(retrieved_docs, 1):
                print(f"  [{i}] {doc.get('title', 'ì œëª© ì—†ìŒ')} (ìœ ì‚¬ë„: {doc['similarity']:.3f})")
            print()
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = create_rag_prompt(query, retrieved_docs)
        
        # 3. GPT-4oë¡œ ì‘ë‹µ ìƒì„± (JSON ì‘ë‹µ)
        print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n")
        print("-" * 70)
        
        # ğŸš¨ JSON ì¶œë ¥ì„ ìœ„í•´ stream ì¸ìˆ˜ëŠ” ë¬´ì‹œí•˜ê³  Falseë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.
        json_result = self.generator.generate(
            prompt,
            temperature=temperature,
            stream=False 
        )
        print("-" * 70 + "\n")
        
        # ì—ëŸ¬ ì²˜ë¦¬
        if not json_result:
             # LLM ì‘ë‹µ ì‹¤íŒ¨ ì‹œ ì„ì‹œ ì§ˆë¬¸ 4ê°œì™€ í•¨ê»˜ ë°˜í™˜
            return {
                "answer": 'ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (API ì˜¤ë¥˜)',
                "sources": [],
                "query_title": query,
                "follow_up_questions": [ 
                    "ì§€ì›ê¸ˆ ì‹ ì²­ ê¸°ê°„ì´ ê¶ê¸ˆí•´ìš”.",
                    "ì‹ ì²­ ìê²© ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "ë‹¤ë¥¸ ì§€ì—­ì˜ ìœ ì‚¬ ì‚¬ì—…ì´ ìˆë‚˜ìš”?",
                    "ì œì¶œí•´ì•¼ í•  ì„œë¥˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
                ]
            }
        
        # 4. ê²°ê³¼ ë°˜í™˜ (ğŸš¨ LLMì´ ë°˜í™˜í•œ JSON ê²°ê³¼ ì‚¬ìš© ğŸš¨)
        # ----------------------------------------------------------------------
        
        # 4-1. ì¶œì²˜ ë°ì´í„°(sources) í˜•ì‹ ë³€í™˜
        final_sources = []
        for doc in retrieved_docs:
            source_path = doc.get('source', '')
            
            # source ë³€í™˜: íŒŒì¼ ê²½ë¡œì—ì„œ íŒŒì¼ ì´ë¦„(í™•ì¥ì ì œì™¸)ë§Œ ì¶”ì¶œ
            source_name = Path(source_path).stem if source_path else 'N/A'
            
            # snippet ìƒì„±: contentì˜ ì²˜ìŒ 150ìë§Œ ì¶”ì¶œ
            content = doc.get('content', doc.get('text', 'ë‚´ìš©ì—†ìŒ'))
            snippet = content[:150].strip() + "..."
            
            final_sources.append({
                "title": doc.get('title', 'ì œëª© ì—†ìŒ'),
                "source": source_name, 
                "url": doc.get('url', 'N/A'),
                "snippet": snippet
            })

        # 4-2. ìµœì¢… API ì‘ë‹µ êµ¬ì„±
        # LLMì´ ìƒì„±í•œ JSONì˜ í•„ë“œë¥¼ ì‚¬ìš©í•˜ê³ , sourcesë§Œ ë³„ë„ë¡œ êµ¬ì„±í•˜ì—¬ í•©ì¹©ë‹ˆë‹¤.
        final_response = {
            "answer": json_result.get("botResponse", "ë‹µë³€ í…ìŠ¤íŠ¸ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."), 
            "sources": final_sources, 
            "query_title": json_result.get("queryTitle", query),
            "follow_up_questions": json_result.get("followUpQuestions", [])
        }
        
        # 4-3. í›„ì† ì§ˆë¬¸ì´ 4ê°œê°€ ì•„ë‹Œ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ (LLM ì˜¤ë¥˜ ì‹œ) ì•ˆì „ ì¥ì¹˜
        if len(final_response["follow_up_questions"]) < 4:
             print("âš ï¸  LLMì´ 4ê°œì˜ ì§ˆë¬¸ì„ ëª¨ë‘ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ ê°œìˆ˜ë¥¼ ë³´ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


        return final_response


# ============================================================
# 7. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (main í•¨ìˆ˜ë„ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë„ë¡ ìˆ˜ì •)
# ============================================================

def main():
    # ... (ì¤‘ëµ: ê²½ë¡œ ë° API í‚¤ í™•ì¸ ë¡œì§)

    if not os.path.exists(METADATA_PATH):
        print(f"âŒ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {METADATA_PATH}")
        return
    
    try:
        # RAG ì±—ë´‡ ì´ˆê¸°í™”
        chatbot = PolicyRAGChatbot(
            model_path=FINETUNED_MODEL_PATH,
            index_path=FAISS_INDEX_PATH,
            metadata_path=METADATA_PATH,
            api_key=OPENAI_API_KEY,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        
        # ëŒ€í™”í˜• ë£¨í”„
        print("ğŸ’¬ ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')\n")
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input("ğŸ‘¤ ë‹¹ì‹ : ").strip()
                
                if not user_input:
                    continue
                
                # ì¢…ë£Œ ëª…ë ¹
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                
                print()  # ë¹ˆ ì¤„
                
                # ë‹µë³€ ìƒì„±
                result = chatbot.answer(
                    query=user_input,
                    top_k=5,
                    temperature=0.7,
                    stream=False, # JSON ì¶œë ¥ ë•Œë¬¸ì— Falseë¡œ ê³ ì •
                    show_sources=True
                )
                
                # ê²°ê³¼ ì¶œë ¥ (JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥)
                if result:
                    print("ğŸ“ ìµœì¢… ë‹µë³€:")
                    print(result['answer'])
                    print("\nâ“ í›„ì† ì§ˆë¬¸:")
                    for q in result.get('follow_up_questions', []):
                         print(f" - {q}")
                    
                print("\n" + "="*70 + "\n")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")
                continue
    
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


# ============================================================
# 8. ë‹¨ì¼ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ìˆ˜ì • ë°˜ì˜)
# ============================================================

def test_single_query(query: str):
    """ë‹¨ì¼ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ (ëŒ€í™”í˜• ë£¨í”„ ì—†ì´)"""
    
    if not OPENAI_API_KEY:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # RAG ì±—ë´‡ ì´ˆê¸°í™”
    chatbot = PolicyRAGChatbot(
        model_path=FINETUNED_MODEL_PATH,
        index_path=FAISS_INDEX_PATH,
        metadata_path=METADATA_PATH,
        api_key=OPENAI_API_KEY,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # ë‹µë³€ ìƒì„±
    result = chatbot.answer(
        query=query,
        top_k=5,
        temperature=0.7,
        stream=False,  # ìŠ¤íŠ¸ë¦¬ë° ì—†ì´
        show_sources=True
    )
    
    # ê²°ê³¼ ì¶œë ¥
    if result:
        print("ğŸ“ ìµœì¢… ë‹µë³€:")
        print(result['answer'])
        print("\nâ“ í›„ì† ì§ˆë¬¸:")
        for q in result.get('follow_up_questions', []):
             print(f" - {q}")
        print("\n" + "="*70 + "\n")


# ============================================================
# ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    # ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰
    main()
    
    # ë˜ëŠ” ë‹¨ì¼ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ (ì•„ë˜ ì£¼ì„ í•´ì œ)
    # test_single_query("ì¤‘ì†Œê¸°ì—…ì„ ìœ„í•œ R&D ì§€ì› ì‚¬ì—…ì´ ìˆë‚˜ìš”?")