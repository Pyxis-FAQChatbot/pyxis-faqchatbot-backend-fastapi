#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì •ì±… ì§€ì› ì•ˆë‚´ RAG ì±—ë´‡
- FAISS ì¸ë±ìŠ¤ ê¸°ë°˜ ê²€ìƒ‰
- OpenAI GPT-4oë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„±
- íŒŒì¸íŠœë‹ëœ BAAI/bge-m3 ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
"""

import os
import json
import faiss
import numpy as np
from pathlib import Path
from typing import List, Dict, Any
from openai import OpenAI
from transformers import AutoModel, AutoTokenizer
import torch

# ============================================================
# 1. ê²½ë¡œ ì„¤ì • (ğŸš¨ Path ê°ì²´ ëŒ€ì‹  ìˆœìˆ˜ ë¬¸ìì—´ë¡œ ìˆ˜ì •ë¨ ğŸš¨)
# ============================================================

# ë°”íƒ•í™”ë©´ ê²½ë¡œ ìë™ ê°ì§€ (ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±°)
# DESKTOP_PATH = Path.home() / "Desktop"

# íŒŒì¼ ê²½ë¡œ (Path ê°ì²´ì—ì„œ ë¬¸ìì—´ë¡œ ë³€ê²½)
FINETUNED_MODEL_PATH = "C:\\Users\\user\\Desktop\\bge-m3-sft"
FAISS_INDEX_PATH = "C:\\Users\\user\\Desktop\\policy_faiss.index"
METADATA_PATH = "C:\\Users\\user\\Desktop\\metadata.json"

# OpenAI API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    print("âš ï¸  ê²½ê³ : OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. í„°ë¯¸ë„ì—ì„œ: export OPENAI_API_KEY='your-api-key'")
    print("2. ì½”ë“œì—ì„œ ì§ì ‘: OPENAI_API_KEY = 'your-api-key'")
    # ë˜ëŠ” ì—¬ê¸°ì— ì§ì ‘ ì…ë ¥ (ë³´ì•ˆìƒ ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
    # OPENAI_API_KEY = "your-api-key-here"

print("="*70)
print("ğŸ¤– ì •ì±… ì§€ì› ì•ˆë‚´ RAG ì±—ë´‡")
print("="*70)
# ì¶œë ¥ ì‹œì—ëŠ” Path ê°ì²´ì˜ .exists() ëŒ€ì‹  os.path.existsë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • í•„ìš”
print(f"ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {FINETUNED_MODEL_PATH}")
print(f"ğŸ“‚ FAISS ì¸ë±ìŠ¤: {FAISS_INDEX_PATH}")
print(f"ğŸ“‚ ë©”íƒ€ë°ì´í„°: {METADATA_PATH}")
print("="*70 + "\n")


# ============================================================
# 2. íŒŒì¸íŠœë‹ëœ ì„ë² ë”© ëª¨ë¸ ë¡œë” í´ë˜ìŠ¤
# ============================================================

class FineTunedEmbedder:
    """íŒŒì¸íŠœë‹ëœ BAAI/bge-m3 ì„ë² ë”© ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: str = None):
        """
        Args:
            model_path: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
            device: 'cuda' ë˜ëŠ” 'cpu' (Noneì´ë©´ ìë™ ê°ì§€)
        """
        print("ğŸ“¦ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        self.model = AutoModel.from_pretrained(model_path, local_files_only=True)
        self.model.to(self.device)
        self.model.eval()
        
        print(f"  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}\n")
    
    def encode(self, texts: List[str], batch_size: int = 32, max_length: int = 512) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            numpy array (n_texts, embedding_dim)
        """
        embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # í† í°í™”
                encoded = self.tokenizer(
                    batch_texts,
                    max_length=max_length,
                    padding=True,
                    truncation=True,
                    return_tensors='pt'
                )
                
                # GPUë¡œ ì´ë™
                input_ids = encoded['input_ids'].to(self.device)
                attention_mask = encoded['attention_mask'].to(self.device)
                
                # ì„ë² ë”© ìƒì„±
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                
                # CLS í† í° ì¶”ì¶œ ë° ì •ê·œí™”
                cls_embeddings = outputs.last_hidden_state[:, 0, :]
                cls_embeddings = torch.nn.functional.normalize(cls_embeddings, p=2, dim=1)
                
                # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
                embeddings.append(cls_embeddings.cpu().numpy())
        
        return np.vstack(embeddings)


# ============================================================
# 3. FAISS ê²€ìƒ‰ê¸° í´ë˜ìŠ¤
# ============================================================

class FAISSRetriever:
    """FAISS ì¸ë±ìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, index_path: str, metadata_path: str, embedder: FineTunedEmbedder):
        """
        Args:
            index_path: FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
            metadata_path: ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ
            embedder: ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        print("ğŸ“š FAISS ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self.index = faiss.read_index(str(index_path))
        print(f"  - ì¸ë±ìŠ¤ í¬ê¸°: {self.index.ntotal:,}ê°œ ë¬¸ì„œ")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        print(f"  - ë©”íƒ€ë°ì´í„°: {len(self.metadata):,}ê°œ í•­ëª©")
        
        # ì„ë² ë”© ëª¨ë¸
        self.embedder = embedder
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ FAISS ì¸ë±ìŠ¤ë¥¼ GPUë¡œ ì´ë™
        if torch.cuda.is_available() and faiss.get_num_gpus() > 0:
            print("  - FAISS GPU ëª¨ë“œ í™œì„±í™”")
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
        
        print("  âœ… FAISS ê²€ìƒ‰ê¸° ì¤€ë¹„ ì™„ë£Œ\n")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ top-k ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ metadata + score)
        """
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedder.encode([query])

        query_embedding = query_embedding.astype('float32')

        print(f"  ğŸ” ì¿¼ë¦¬ ë²¡í„° ì°¨ì›: {query_embedding.shape[1]}")
        print(f"  ğŸ” ì¿¼ë¦¬ ë²¡í„° Dtype: {query_embedding.dtype}")

        # FAISS ê²€ìƒ‰ (L2 ê±°ë¦¬)
        distances, indices = self.index.search(query_embedding, top_k)
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.metadata):
                result = self.metadata[idx].copy()
                result['similarity']=float(dist)
                result['score']=float(dist)
                results.append(result)
        
        return results


# ============================================================
# 4. RAG í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
# ============================================================

def create_rag_prompt(query: str, retrieved_docs: List[Dict[str, Any]]) -> str:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì¿¼ë¦¬ë¥¼ ê²°í•©í•˜ì—¬ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    context_parts = []
    MAX_CONTENT_LENGTH = 3000

    for i, doc in enumerate(retrieved_docs, 1):
        context_parts.append(f"[ë¬¸ì„œ {i}]")
        context_parts.append(f"ì œëª©: {doc.get('title', 'ì œëª© ì—†ìŒ')}")
        
        full_content = doc.get('content', doc.get('text', 'ë‚´ìš©ì—†ìŒ'))
        truncated_content = full_content[:MAX_CONTENT_LENGTH] + ("..." if len(full_content) > MAX_CONTENT_LENGTH else "")
        context_parts.append(f"ë‚´ìš©: {truncated_content}")
        
        if 'source' in doc:
            context_parts.append(f"ì¶œì²˜: {doc['source']}")
        context_parts.append("") # ë¹ˆ ì¤„
    
    context = "\n".join(context_parts)
    
    # ğŸš¨ğŸš¨ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìˆ˜ì •: í˜ë¥´ì†Œë‚˜ ë° ìŠ¤íƒ€ì¼ ì ìš© ğŸš¨ğŸš¨
    system_instruction = f"""
    ë„ˆëŠ” ì •ì±…ì´ë‚˜ ë²•ë¥  ìš©ì–´ë¥¼ ì–´ë ¤ì›Œí•˜ëŠ” ì¹œêµ¬ì—ê²Œ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ì¹œì ˆí•œ ì •ì±… ì „ë¬¸ ì¡°ì–¸ìì•¼.
    
    --- LLM í–‰ë™ ë° ì‘ë‹µ ê·œì¹™ ---
    1. **ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼:** **ë‹µë³€ì€ ë°˜ë“œì‹œ í•­ëª© ì œëª©(`ì§€ì› ëŒ€ìƒ:`, `ì‹ ì²­ ë°©ë²•:`, `ë¬¸ì˜ì²˜:`)ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ê¸€ë¡œ ë…¹ì—¬ë‚´ì•¼ í•´.** (ì¹œê·¼í•œ í¸ì§€ë‚˜ ë©”ì‹œì§€ í˜•íƒœ)
    2. **í˜ë¥´ì†Œë‚˜ì™€ ë§íˆ¬:** ë‹µë³€ ì „ì²´ì—ì„œ 'ì¹œêµ¬ì²˜ëŸ¼', 'ì„ ë°°ì²˜ëŸ¼' ì¹œê·¼í•˜ê³  ì‰½ê²Œ ë§í•´ì¤˜.
    3. **ìš©ì–´ ì„¤ëª…:** 'ì„¸ë¬´', 'íšŒê³„', 'ë²•ë¥ ', 'ìœµì', 'ì²´ë‚©' ë“± ì–´ë ¤ìš´ ì „ë¬¸ ìš©ì–´ëŠ” (ê´„í˜¸ ì•ˆì— ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ) ë°˜ë“œì‹œ ì„¤ëª…í•´ ì¤˜ì•¼ í•´.
    4. **URL ê²€ìƒ‰ ê·œì¹™:** ë‹µë³€ì— ì›¹ì‚¬ì´íŠ¸ë‚˜ í”Œë«í¼ ì´ë¦„ì´ í¬í•¨ë˜ë©´, Google ê²€ìƒ‰ì„ ì‚¬ìš©í•´ì„œ ê³µì‹ URLì„ ì°¾ì•„ í•˜ì´í¼ë§í¬ í˜•ì‹([í”Œë«í¼ ì´ë¦„](URL))ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ì²¨ë¶€í•´ì•¼ í•´.
    5. **ì‹¤í–‰ë ¥ í‚¤ìš°ê¸° (Next Step):** ë‹µë³€ì˜ ë§ˆì§€ë§‰ì—ëŠ” ì†Œìƒê³µì¸ ì¹œêµ¬ê°€ ë°”ë¡œ ì›€ì§ì¼ ìˆ˜ ìˆë„ë¡, ê°€ì¥ ë¹ ë¥´ê³  êµ¬ì²´ì ì¸ ë‹¤ìŒ í–‰ë™ ë‹¨ê³„ë¥¼ ë”± í•˜ë‚˜ë§Œ ì½• ì§‘ì–´ ì œì‹œí•´ ì¤˜. 'ë°”ë¡œ í•´ë³´ì!', 'ì´ê²ƒë¶€í„° ì‹œì‘í•˜ì!' ê°™ì€ ë…ë ¤í•˜ëŠ” ë§íˆ¬ë¡œ ë§ˆë¬´ë¦¬í•´ì•¼ í•´.
    6. **ë¬¸ì„œ ê¸°ë°˜:** ì œê³µëœ ë¬¸ì„œì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë ¤ì›Œ. ë” ì°¾ì•„ë³´ì!"ë¼ê³  ë§í•´ì¤˜.
    
    --- ë ---
    """
    
    # ğŸš¨ğŸš¨ ì‚¬ìš©ì ì½˜í…ì¸  (User Content) ì •ì˜ ğŸš¨ğŸš¨
    # LLMì´ ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•´ì•¼ í•  ìë£Œì™€ ìµœì¢… ì§ˆë¬¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
    user_content = f"""
    --- ë‹µë³€ ì°¸ê³  ìë£Œ ---
    {context}
    
    ì‚¬ìš©ì ì§ˆë¬¸: {query}
    
    ë‹µë³€:
    """
    
    # SYSTEM ì—­í• ê³¼ USER ì—­í• ì˜ ë©”ì‹œì§€ë¥¼ ê²°í•©í•˜ì—¬ ë°˜í™˜ (API í˜¸ì¶œ ì‹œ ë¶„ë¦¬ë¨)
    # LLMì´ 'SYSTEM' ì˜ì—­ì€ ì¶œë ¥í•˜ì§€ ì•Šê³  'USER' ì˜ì—­ì— ëŒ€í•œ ë‹µë³€ë§Œ í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
    return f"{system_instruction.strip()}\n{user_content.strip()}"


# ============================================================
# 5. OpenAI GPT-4o ì‘ë‹µ ìƒì„±ê¸°
# ============================================================

class GPT4oGenerator:
    """OpenAI GPT-4o ê¸°ë°˜ ì‘ë‹µ ìƒì„±ê¸°"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        """
        Args:
            api_key: OpenAI API í‚¤
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸: gpt-4o)
        """
        print("ğŸ¤– OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        self.client = OpenAI(api_key=api_key)
        self.model = model
        print(f"  - ëª¨ë¸: {model}")
        print("  âœ… ì¤€ë¹„ ì™„ë£Œ\n")
    
    def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        stream: bool = False
    ) -> str:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„± (ì‹œìŠ¤í…œ ì—­í•  ë¶„ë¦¬ ì ìš©)
        """
        try:
            # 1. í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬: ì‹œìŠ¤í…œ ì§€ì¹¨ê³¼ ì‚¬ìš©ì ì½˜í…ì¸ ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
            if "--- ë‹µë³€ ì°¸ê³  ìë£Œ ---" in prompt:
                # ê·œì¹™ ë° í˜ë¥´ì†Œë‚˜ (ì‹œìŠ¤í…œ ë©”ì‹œì§€)
                system_content = prompt.split("--- ë‹µë³€ ì°¸ê³  ìë£Œ ---")[0].strip()
                # ì°¸ê³  ìë£Œ ë° ì‚¬ìš©ì ì§ˆë¬¸ (ì‚¬ìš©ì ë©”ì‹œì§€)
                user_content = prompt.split("--- ë‹µë³€ ì°¸ê³  ìë£Œ ---")[1].strip()
                
                messages = [
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": user_content}
                ]
            else:
                # ì•ˆì „ ì¥ì¹˜: ë¶„ë¦¬ê°€ ì•ˆ ë  ê²½ìš° ê¸°ì¡´ëŒ€ë¡œ user contentì— í†µí•©
                messages = [{"role": "user", "content": prompt}]

            # 2. API í˜¸ì¶œ
            if stream:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages, # ğŸ‘ˆ ë¶„ë¦¬ëœ ë©”ì‹œì§€ ì‚¬ìš©
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stream=True
                )
                
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content is not None:
                        content = chunk.choices[0].delta.content
                        print(content, end="", flush=True)
                        full_response += content
                print() # ì¤„ë°”ê¿ˆ
                return full_response
            else:
                # ì¼ë°˜ ëª¨ë“œ
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages, # ğŸ‘ˆ ë¶„ë¦¬ëœ ë©”ì‹œì§€ ì‚¬ìš©
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return response.choices[0].message.content
                
        except Exception as e:
            print(f"âŒ OpenAI API ì˜¤ë¥˜: {e}")
            return None


# ============================================================
# 6. RAG ì±—ë´‡ í´ë˜ìŠ¤ (ì „ì²´ íŒŒì´í”„ë¼ì¸)
# ============================================================

class PolicyRAGChatbot:
    """ì •ì±… ì§€ì› ì•ˆë‚´ RAG ì±—ë´‡"""
    
    def __init__(
        self,
        model_path: str,
        index_path: str,
        metadata_path: str,
        api_key: str,
        device: str = None
    ):
        """
        Args:
            model_path: íŒŒì¸íŠœë‹ëœ ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ
            index_path: FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
            metadata_path: ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ
            api_key: OpenAI API í‚¤
            device: ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        """
        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.embedder = FineTunedEmbedder(str(model_path), device)
        
        # 2. FAISS ê²€ìƒ‰ê¸° ë¡œë“œ
        self.retriever = FAISSRetriever(index_path, metadata_path, self.embedder)
        
        # 3. GPT-4o ìƒì„±ê¸° ì´ˆê¸°í™”
        self.generator = GPT4oGenerator(api_key)
        
        print("="*70)
        print("âœ… RAG ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!")
        print("="*70 + "\n")
    
    def answer(
        self,
        query: str,
        top_k: int = 5,
        temperature: float = 0.7,
        stream: bool = False,
        show_sources: bool = True
    ) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (FastAPI API ìŠ¤í™ ì¤€ìˆ˜)
        """
        print(f"ğŸ’­ ì§ˆë¬¸: {query}\n")
        
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        print(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘... (top-{top_k})")
        retrieved_docs = self.retriever.search(query, top_k)
        print(f"  âœ… {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ\n")
        
        # ì¶œì²˜ í‘œì‹œ
        if show_sources:
            print("ğŸ“š ì°¸ê³  ë¬¸ì„œ:")
            for i, doc in enumerate(retrieved_docs, 1):
                print(f"  [{i}] {doc.get('title', 'ì œëª© ì—†ìŒ')} (ìœ ì‚¬ë„: {doc['similarity']:.3f})")
            print()
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = create_rag_prompt(query, retrieved_docs)
        
        # 3. GPT-4oë¡œ ì‘ë‹µ ìƒì„±
        print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n")
        print("-" * 70)
        answer = self.generator.generate(
            prompt,
            temperature=temperature,
            stream=stream
        )
        print("-" * 70 + "\n")
        
        # 4. ê²°ê³¼ ë°˜í™˜ (ğŸš¨ ìµœì¢… ìˆ˜ì •ëœ ë¶€ë¶„ ğŸš¨)
        # ----------------------------------------------------------------------
        # 4. ê²°ê³¼ ë°˜í™˜ (API ìŠ¤í™ì— ë§ê²Œ ë°ì´í„° ë³€í™˜ ë° í˜•ì‹ ë§ì¶”ê¸°)
        # ----------------------------------------------------------------------
        
        # 4-1. ì¶œì²˜ ë°ì´í„°(sources) í˜•ì‹ ë³€í™˜
        final_sources = []
        for doc in retrieved_docs:
            source_path = doc.get('source', '')
            
            # source ë³€í™˜: íŒŒì¼ ê²½ë¡œì—ì„œ íŒŒì¼ ì´ë¦„(í™•ì¥ì ì œì™¸)ë§Œ ì¶”ì¶œ
            source_name = Path(source_path).stem
            
            # snippet ìƒì„±: contentì˜ ì²˜ìŒ 150ìë§Œ ì¶”ì¶œ
            content = doc.get('content', doc.get('text', 'ë‚´ìš©ì—†ìŒ'))
            snippet = content[:150].strip() + "..."
            
            final_sources.append({
                "title": doc.get('title', 'ì œëª© ì—†ìŒ'),
                "source": source_name, 
                "url": doc.get('url', 'N/A'), # metadataì— urlì´ ì—†ë‹¤ë©´ N/A
                "snippet": snippet
            })

        # 4-2. LLMì´ ìƒì„±í•˜ì§€ ì•ŠëŠ” query_title ë° follow_up_questionsì— ì„ì‹œ ê°’ í• ë‹¹
        # (ì‹¤ì œ êµ¬í˜„ ì‹œ LLMì—ê²Œ JSONìœ¼ë¡œ ìš”ì²­í•˜ì—¬ ì¶”ì¶œí•´ì•¼ í•¨)
        query_title = f"ì§ˆë¬¸ ìš”ì•½: {query[:20]}..."
        follow_up_questions = ["ì‹ ì²­ ìê²© ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?", "ë‹¤ë¥¸ ê´€ë ¨ ì‚¬ì—…ë„ ì°¾ì•„ë³¼ ìˆ˜ ìˆë‚˜ìš”?"]

        return {
            "answer": answer, # ğŸ‘ˆ LLMì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€ í…ìŠ¤íŠ¸
            "sources": final_sources, # ğŸ‘ˆ API í˜•ì‹ì— ë§ê²Œ ë³€í™˜ëœ ì¶œì²˜ ë¦¬ìŠ¤íŠ¸
            "query_title": query_title, # ğŸ‘ˆ ì„ì‹œë¡œ ìƒì„±ëœ ì§ˆë¬¸ ìš”ì•½
            "follow_up_questions": follow_up_questions # ğŸ‘ˆ ì„ì‹œë¡œ ìƒì„±ëœ í›„ì† ì§ˆë¬¸
        }


# ============================================================
# 7. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # API í‚¤ í™•ì¸
    if not OPENAI_API_KEY:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    
    if not os.path.exists(FAISS_INDEX_PATH): # Path.exists() ëŒ€ì‹  os.path.exists() ì‚¬ìš©
        print(f"âŒ FAISS ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FAISS_INDEX_PATH}")
        return
    
    if not os.path.exists(METADATA_PATH): # Path.exists() ëŒ€ì‹  os.path.exists() ì‚¬ìš©
        print(f"âŒ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {METADATA_PATH}")
        return
    
    try:
        # RAG ì±—ë´‡ ì´ˆê¸°í™”
        chatbot = PolicyRAGChatbot(
            model_path=str(FINETUNED_MODEL_PATH),
            index_path=str(FAISS_INDEX_PATH),
            metadata_path=str(METADATA_PATH),
            api_key=OPENAI_API_KEY,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        
        # ëŒ€í™”í˜• ë£¨í”„
        print("ğŸ’¬ ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')\n")
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input("ğŸ‘¤ ë‹¹ì‹ : ").strip()
                
                if not user_input:
                    continue
                
                # ì¢…ë£Œ ëª…ë ¹
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                
                print()  # ë¹ˆ ì¤„
                
                # ë‹µë³€ ìƒì„±
                result = chatbot.answer(
                    query=user_input,
                    top_k=5,
                    temperature=0.7,
                    stream=True,  # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
                    show_sources=True
                )
                
                print("\n" + "="*70 + "\n")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")
                continue
    
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


# ============================================================
# 8. ë‹¨ì¼ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ============================================================

def test_single_query(query: str):
    """ë‹¨ì¼ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ (ëŒ€í™”í˜• ë£¨í”„ ì—†ì´)"""
    
    if not OPENAI_API_KEY:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # RAG ì±—ë´‡ ì´ˆê¸°í™”
    chatbot = PolicyRAGChatbot(
        model_path=str(FINETUNED_MODEL_PATH),
        index_path=str(FAISS_INDEX_PATH),
        metadata_path=str(METADATA_PATH),
        api_key=OPENAI_API_KEY,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # ë‹µë³€ ìƒì„±
    result = chatbot.answer(
        query=query,
        top_k=5,
        temperature=0.7,
        stream=False,  # ìŠ¤íŠ¸ë¦¬ë° ì—†ì´
        show_sources=True
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("ğŸ“ ìµœì¢… ë‹µë³€:")
    print(result['answer'])


# ============================================================
# ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    # ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰
    main()
    
    # ë˜ëŠ” ë‹¨ì¼ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ (ì•„ë˜ ì£¼ì„ í•´ì œ)
    # test_single_query("ì¤‘ì†Œê¸°ì—…ì„ ìœ„í•œ R&D ì§€ì› ì‚¬ì—…ì´ ìˆë‚˜ìš”?")